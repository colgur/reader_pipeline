Friday, June 05 2009
We'll get a feeds.google.reader.Feed(title)
   e.g. prog_reddit = feeds.google.reader.Feed('Programming Reddit')

feeds.google.reader.Feed attributes: title, identifier, unread_count

Have a flow like this:
   # basic init: no i/o
   prog_reddit = feeds.google.reader.atom.feeds('username', 'password')
   # pick-up: identifier, unread_count, and reading_list
   prog_reddit[i].refresh
   # might want to parameterize Feed.reading_list()
   unread_count = prog_reddit[i].unread_count
   # reading_list is raw rss
   reading_list = prog_reddit[i].reading_list

   prog_reddit.reading_list() is rss: pass directly to feedparser.parse()
      might be handy to integrate feedparser:
         prog_reddit.parse()

Saturday, May 23 2009
Should use Google Reader API so there is no need for local feed store

Using NLTK to deliver the top words
   top phrase might be interesting but interaction would be more productive
   a tag cloud or heat map

So it goes like this now:
   1. Source Reader for certain feeds
   2. Produce a cloud of terms
   3. Click on term in cloud to view feed entries

Monday, May 18 2009
Going to take the oldest feed file and update it
   Use a python list of objects corresponding to the most primitive RSS 2.0 feed:
      [((channel:title)([(item:title, item:link)]))]
   or hierarchally:
      channel:title
         item
            title
            link
         item
            title
            link
         ...

Could be a little more general and use sets:
   Oldest feed will be a subset of newer so we just pickle the union