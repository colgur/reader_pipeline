Sunday, June 21 2009
Label "version_1"
   Pretty interesting feature set now:
      1. Expat integration
      2. FeedParser integration
      3. NLTK integration
      4. Google Reader API integration
   Pulling data from Reader Repository and sampling a pretty large (>2000) corpus of Feed Titles.
   Should probably clean-up and profile the code a little

Title Corpus alone is not interesting: NLTK analysis needs to be a little more sophisticated
   Probably need to inform with a UI

Performance isn't great
   Thinking of moving to Google AppEngine.
   Should profile the code a little: might just be network latency but algorithms might need tweaking

Thinking of a push to GitHub
   utils.fsm does some pretty cool Expat integration (IMHO).
   Would like some feedback on that code, maybe some better ideas.

Could probably use some database integration
   No need to go out for the Feeds every time the Titles need to analyzed: another reason to use AppEngine

Been thinking of some real-time Twitter feed analysis
   Weight words according to time: if 'Iran' occurs in 10 different tweets within 15 minutes of each other then it becomes interesting for the analysis and starts a process of running deeper (further back in time)
      I'm sure there is some analysis in there that would help me spot trends
   Not restricted to Twitter. A similar analysis could be applied to any bookmarking feed

Monday, June 08 2009
List API could be similar:
   subscriptions = feeds.google.reader.subscriptions
   title = subscriptions[i].title

Friday, June 05 2009
We'll get a feeds.google.reader.Feed(title)
   e.g. prog_reddit = feeds.google.reader.Feed('Programming Reddit')

feeds.google.reader.Feed attributes: title, identifier, unread_count

Have a flow like this:
   # basic init: no i/o
   reader_feeds = feeds.google.reader.atom.feeds('username', 'password')
   prog_reddit = reader_feeds[i]

   # pick-up: identifier, unread_count, and reading_list
   prog_reddit.refresh

   # might want to parameterize Feed.reading_list()
   unread_count = prog_reddit.unread_count

   # reading_list is raw rss
   reading_list = prog_reddit.reading_list

   prog_reddit.reading_list() is rss: pass directly to feedparser.parse()
      might be handy to integrate feedparser:
         prog_reddit.parse()

Saturday, May 23 2009
Should use Google Reader API so there is no need for local feed store

Using NLTK to deliver the top words
   top phrase might be interesting but interaction would be more productive
   a tag cloud or heat map

So it goes like this now:
   1. Source Reader for certain feeds
   2. Produce a cloud of terms
   3. Click on term in cloud to view feed entries

Monday, May 18 2009
Going to take the oldest feed file and update it
   Use a python list of objects corresponding to the most primitive RSS 2.0 feed:
      [((channel:title)([(item:title, item:link)]))]
   or hierarchally:
      channel:title
         item
            title
            link
         item
            title
            link
         ...

Could be a little more general and use sets:
   Oldest feed will be a subset of newer so we just pickle the union